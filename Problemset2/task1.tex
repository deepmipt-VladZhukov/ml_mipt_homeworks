\documentclass[11pt,a4paper]{article}
% \renewcommand\normalsize{\fontsize{12}{18.0pt}\selectfont}
\usepackage[utf8]{inputenc}
\usepackage[T2A]{fontenc}
\usepackage[russian]{babel}
\usepackage{graphicx}
\usepackage{amsmath,amssymb,amsthm}
\usepackage{epsfig}
\usepackage{import}
\usepackage{wrapfig}
\usepackage{epigraph}
\usepackage{verbatim}
\usepackage{soul}
\usepackage[usenames]{color}
\usepackage{listings}
\usepackage{pdfpages}

\usepackage[pdf]{graphviz}

%  ln code
\usepackage{amsmath}
\usepackage{mleftright}

\newcommand{\lnn}[1]{%
  \ln\left(#1\right)%
}

\newcommand{\lnb}[1]{%
  \ln\mleft(#1\mright)%
}
% code for ln

\pagestyle{empty}

\hoffset=-15mm    %по горизонтали влево на \hoffset
\voffset=-37.5mm  %по вертикали вверх на \voffset
\textheight=275mm
\textwidth=155mm


\newcommand{\eps}{\varepsilon}
\renewcommand{\phi}{\varphi}
\newcommand{\Impl}{\ensuremath{\Rightarrow}}
\newcommand{\RRR}{\overline{\mathbb{R}}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\nl}{\newline}

\usepackage{ifthen}
\newcommand\ifnonempty[2]{\ifthenelse{\equal{#1}{}}{}{#2}}
% Команда \task для условий задач с одним необязательным аргументом
% \defin определения, \exerc упражнения \prop предложения \theor теорема
\newcounter{task}
\newcounter{defin}
\newcounter{prop}
\newcounter{thm}
\newcounter{lem}
\newcommand{\task}[1][]{\smallskip\par\hangafter=1\normalsize\textbf{Задача \refstepcounter{task}\thetask\ifnonempty{#1}{ (#1)}.}~}
\newcommand{\defin}[1][]{\smallskip\par\hangafter=1\normalsize\textbf{Определение \refstepcounter{defin}\thedefin\ifnonempty{#1}{ (#1)}.}~}
\newcommand{\exerc}[1][]{\smallskip\par\hangafter=1\normalsize\textbf{Упражнение \refstepcounter{task}\thetask\ifnonempty{#1}{ (#1)}.}~}
\newcommand{\prop}[1][]{\smallskip\par\hangafter=1\normalsize\textbf{Предложение \refstepcounter{prop}\theprop\ifnonempty{#1}{ (#1)}.}~}
\newcommand{\thm}[1][]{\smallskip\par\hangafter=1\normalsize\textbf{Теорема \refstepcounter{thm}\thethm\ifnonempty{#1}{ (#1)}.}~}
\newcommand{\lem}[1][]{\smallskip\par\hangafter=1\normalsize\textbf{Лемма \refstepcounter{lem}\thelem\ifnonempty{#1}{ (#1)}.}~}
% \setcounter{thm}{2}

\begin{document}
\begin{center}
\Huge {
\noindent
\textbf{1 Теоретические задачи}
}
\end{center}

\Large {
\textbf {1.1 Ответы в листьях регрессионного дерева}
}
\\
Обозначения:
$$ MSE(X, Y) = \sum_{y_i \in Y} (\xi_i - y_i)^2$$
Где таргеты в листе:
$$ X = \{x_i\}_i^n $$
тестовые таргеты
$$ Y = \{y_i\}_i^m $$
$\xi_i \sim U(X)$ случайная величина - таргет случайного объекта из листа (из условия задачи).
$$ p_{ij} = Pr(\xi_i = x_j) = \frac{1}{n}$$
Тогда матожидание ошибки по MSE можно посчитать как:
$$ \mathbb E MSE = \sum_{k=1}^n \mathbb{E} (MSE(X, Y)|X, Y) \cdot I(X, Y)$$
То есть если мы посчтаем эти суммы для обоих случаев и сравним слагаемые (что я и сделаю дальше), то мы получим искомое неравенство.
В случае сэмплирования ответов из таргета имеем:
\\
$$\mathbb{E}( \sum_{i=1}^m (\xi_i - y_i)^2| X, Y)=$$
$$ = \sum_{i=1}^m \sum_{j=1}^n p_{ij} (\xi_i - y_i)^2 =$$
$$ = \sum_{i=1}^m \sum_{j=1}^n p_{ij} \xi^2 - 2p_{ij} \xi_i y_i + p_{ij}y_i^2 = $$
\begin{center}
$ = \sum_{i=1}^m( \sum_{j=1}^n {\frac{x_j^2}{n}} - \sum_{j=1}^n \frac{2}{n} x_j y_i + y_i^2)$ (*)
\end{center}
В случае выдачи констного среднего ответа по таргетам в листе:
\\
$$ \mathbb \sum_{i=1}^m (\frac{\sum_{j=1}^n x_j}{n} - y_i)^2 = $$
\begin{center}
$ \sum_{i=1}^m ( (\sum_{j=1}^n \frac{x_j}{n})^2 - \sum_{j=1}^n \frac{2}{n} x_j y_i + y_i^2)$ (**)
\end{center}
Получаем (*) > (**), поскольку:
$$ \sum_{j=1}^n \frac{x_j^2}{n} > (\sum_{j=1}^n \frac{x_j}{n})^2 $$

\Large {
\textbf {1.2 Линейные модели в деревьях}
}
Критерий построения разбиений в регрессионном дереве никак не учитывает то, насколько в каждой из дочерних ветвей зависимость близка к линейной.(можно взять точки на прямой с большим расстоянием друг от друга).
Тогда в качестве меры "хорошести"(было ""неоднородности") множества можно использовать, например, MSE модели $a(x)$ обученной на данном множестве, те.:
$$ H(R) = \frac{1}{|R|} \sum_{x_j \in R}(y_i - a(x_i))^2 $$
и подставить этот $H(R)$ в критерий разбиения(в обозначениях лекции):
$$ \frac{|L|}{|Q|} H(L) + \frac{|R|}{|Q|} H(R) $$
\Large {
\textbf {1.3 unsupervised dicision tree}
}
\\
По определению $$H(f) = -\int_{x \in \mathbb{R}^n}^{} f(x) \ln(f(x))dx = $$
$$ = -\mathbb{E}\ln(f(x)) $$
$$ \lnb{\frac{1}{(2\pi)^{\frac{n}{2}} |\Sigma|^{\frac{1}{2}} } e^{ \frac{1}{2} (x - \mu)^T \Sigma^{-1} (x-\mu) }} = $$
$$ = -\lnb{(2\pi)^{\frac{n}{2}} |\Sigma|^{\frac{1}{2}}} - \frac{1}{2} (x - \mu)^T |\Sigma|^{-1} (x - \mu)$$

$$ \mathbb{E} \lnb{f(x)} = -\lnb{(2\pi)^{\frac{1}{2}} |\Sigma|^{\frac{1}{2}}} - \frac{1}{2} \mathbb{E} (x-\mu)^T |\Sigma|^{-1} (x - \mu) = $$

Где $$ \frac{1}{2} \mathbb{E} (x-\mu)^T |\Sigma|^{-1} (x - \mu) =  -\frac{1}{2} tr(\Sigma \Sigma^{-1}) = -\frac{1}{2}n$$

Итого
$$ H(f) =  -\mathbb{E}\ln f(x)  = \frac{n}{2} + \lnb{(2\pi)^{\frac{n}{2}}} |\Sigma|^{\frac{1}{2}} =$$
$$ = \frac{n}{2} + \frac{1}{2} \lnb {(2\pi)^n |\Sigma|} = $$
$$ = \frac{1}{2} \lnb{(2\pi e)^n |\Sigma|} $$
\end{document}
